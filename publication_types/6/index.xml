<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>6 on Yida Wang&#39;s personal website</title>
    <link>https://wangyida.github.io/publication_types/6/</link>
    <description>Recent content in 6 on Yida Wang&#39;s personal website</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2016 Yida Wang</copyright>
    <lastBuildDate>Sun, 12 Mar 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/publication_types/6/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>ZigzagNet: Efficient Deep Learning for Real Object Recognition Based on 3D Models</title>
      <link>https://wangyida.github.io/publication/accv16_wang/</link>
      <pubDate>Sun, 12 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://wangyida.github.io/publication/accv16_wang/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://wangyida.github.io/img/paperArch-eps-converted-to.svg&#34; alt=&#34;paperArch-eps-converted-to&#34; /&gt;
&lt;img src=&#34;https://wangyida.github.io/img/micro_ours-eps-converted-to.svg&#34; alt=&#34;micro_ours-eps-converted-to&#34; /&gt;
More detail can easily be written here using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Face Recognition Using Local PCA Filters</title>
      <link>https://wangyida.github.io/publication/ccbr15_wang/</link>
      <pubDate>Sat, 24 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>https://wangyida.github.io/publication/ccbr15_wang/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;https://wangyida.github.io/img/StandAloneNet.svg&#34; alt=&#34;StandAloneNet&#34; /&gt;
&lt;img src=&#34;https://wangyida.github.io/img/filterFFT.svg&#34; alt=&#34;filterFFT&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;method&#34;&gt;Method&lt;/h2&gt;

&lt;h3 id=&#34;filter-learning&#34;&gt;Filter Learning&lt;/h3&gt;

&lt;h4 id=&#34;the-1st-stage-training&#34;&gt;the 1st Stage Training&lt;/h4&gt;

&lt;p&gt;Like the 1st stage training in PCANet, given $\textit{N}$ training samples $ \textit{I}_{i=1}^{\textit{N}} $ with the same size of $ \textit{m} \times \textit{n} $. The patch size doesn&amp;rsquo;t change in a certain cascading queue, so multiscale filter banks is trained separately.&lt;/p&gt;

&lt;p&gt;Overlapping patches in a particular size of images are collected for training a filter bank using SVD(Singular Value Decomposition). Patches are combined of adjacent pixels. As for the pixels on the edge of a image, we use zero padding to make it still useful. Patch mean is subtracted from each patch before SVD. Then the whole resource for filter training is represented as $\textit{X} = [ \bar{X}_1,\bar{X}_2,&amp;hellip;,\bar{X}_N ] \in R^{k_1k_2*Nmn}$ where the image size of all $\textit{N}$ samples is $m \times n$ and the patch size is $\textit{k}_1 \times \textit{k}_2$.
A single patch feature matrix $ \bar{X}&lt;em&gt;1 $ extracted from an image is formed by a set of vectors as $ [ \bar{x}&lt;/em&gt;{i,1},\bar{x}&lt;em&gt;{i,2},&amp;hellip;,\bar{x}&lt;/em&gt;{i,mn} ] $ where $ \bar{x}_{i,j} $ denotes the $ \textit{j} $th vectorized patches in the $ \textit{i} $th image.&lt;/p&gt;

&lt;p&gt;The meaning of PCA is projecting the original data to another orthogonal space which uses as less basis as possible maximizing the variance of data, so basis in such orthogonal space is selected follows constraint: $\max_{V\in R^{k_1k_2 \times S_1}} ||V^TX||&lt;em&gt;F^2$ while $V^TV = I&lt;/em&gt;{S_1}$ it is solved by eigenvalue decomposition of $XX^T $, so the convolution kernel could be expressed as&lt;/p&gt;

&lt;p&gt;$$ W&lt;em&gt;l^1 = mat&lt;/em&gt;{k_1,k_2}(q_l(XX^T)) \in R^{k_1 \times k_2}, l = 1,2,&amp;hellip;,S_1 $$&lt;/p&gt;

&lt;p&gt;where $S&lt;em&gt;1$ means the number of the set of principle eigenvectors in the 1st layer, $mat&lt;/em&gt;{k_1,k_2}(v)$ is a reshaping function aims to transform $v$ to the size of $k_1 \times k_2$ and function $q_l(XX^T)$ denotes the $l$th principal eigenvector of $XX^T$ or the $l$th left singular vector of $X$.&lt;/p&gt;

&lt;p&gt;Such goal is equivalent to minimize the reconstruction error with a set of eigenvectors as shown below where $I_{S_1}$ is identity matrix with the size of $L_1 \times L_2 $:&lt;/p&gt;

&lt;p&gt;$$
\min_{V\in R^{k_1k_2 \times S_1}}||\textit{X} - \textit{VV}^TX ||&lt;em&gt;F^2, s.t: V^TV = I&lt;/em&gt;{S_1}
    $$&lt;/p&gt;

&lt;h4 id=&#34;concatenated-filter-learning&#34;&gt;Concatenated Filter Learning&lt;/h4&gt;

&lt;p&gt;We optimize the concatenated structure to extract efficient feature by taking full advantage of textures in images and rearranging them properly. For the aim of enriching discriminative features and exploiting benefits from the detachment of the back propagation process, we train the cascaded layers only using the output of the particular convolution kernel. Feature extraction is also formed as a tree-like structure. No input-output pairs uses the same filter banks between two layers. As shown in Figure 2, this feature extraction process is a &amp;lsquo;tree&amp;rsquo; like concatenated structure rather than a &amp;lsquo;chain&amp;rsquo; like one of traditional PCANet. The number of filters in higher stages will be no smaller than previous stage, number of filters will only keep as the same when the remaining dimension of SVD is fixed as always 1 except for the $1st$ stage.&lt;/p&gt;

&lt;p&gt;Assuming that layer $t$ contains $S_t$ filter sets where each set contains $l_t$ filters which could be represented as $l&lt;em&gt;t = \prod\limits&lt;/em&gt;{L=1}^{t}S_L/S&lt;em&gt;t$ and such tree-like concatenate structure has $l&lt;/em&gt;{total} = \prod\limits_{L=1}^{n}S&lt;em&gt;L$ outputs for one sample in all where $n$ is the number of layers used. In such a feature enriched network, benefit comes with the cost for the increasing of convolution kernels, which is $F = \sum&lt;/em&gt;{t = 1}^{n}S_tl_t$ in total; Filters in stage $t$ is trained separately one by one from a single group of outputs of previous layer where the $l $th filter output of the $(t-1)$th stage is $I_i^l = I_i * W_l^{t-1}$ while $ i = 1,2,&amp;hellip;,N$. One set of filters in such standalone training process only uses reconstructed samples from one filter in previous stage as source data, so different filters in previous layer would produce distinguishing learning resource where $*$ denotes 2D convolution and $N$ is always equal to the number of input images due to the standalone training process. Overlapping patches are collected as the same manner as the 1st layer. Patch means are removed from each patch as $\bar{Y}&lt;em&gt;i = [\bar{y}&lt;/em&gt;{i,l,1},\bar{y}&lt;em&gt;{i,l,2},&amp;hellip;,\bar{y}&lt;/em&gt;{i,l,mn}]$ where $\bar{y}_{i,l,j}$ is the $j$th mean-subtracted patch in $I_i^l$. A single filter bank is then obtained from eigenvalue decomposition of ${Y}^l = [\bar{Y}_1^l,\bar{Y}_2^l,&amp;hellip;,\bar{Y}_N^l] \in R^{k_1k&lt;em&gt;2 \times Nmn}$. To make images in different layers having the same size, zeros padding is applied before 2D convolution. Filter is solved as: $W&lt;/em&gt;{l&lt;em&gt;t}^l = mat&lt;/em&gt;{k_1,k_2}(ql(YY^T))\in R^{k_1 \times k_2}$ while $ l_t=1,2,&amp;hellip;,S_l$. As filters is learned standalone, which means that the training data in previous stage is much fewer than PCANet. This means that data would be much fewer the original one for a single branch of filter learning, so less time will be cost on convolution with filter of previous stage. With the help of parallel computing, separate branches of current stage will be computed at the same time. Much training time will be saved.
Feature extracted by multiscale filters is beneficial to recognition. Here we just choose continuous odd numbers for filter scales $k_1$ and $k_2$. Convolution kernels are squares matrix for most of our experiment which means that $k_1 = k_2$.&lt;/p&gt;

&lt;p&gt;\begin{figure}
\centering
\includegraphics[height=3cm]{filterFFT}
\caption{Multi scale Filters(odd-numbered rows) learned in the $1st$ layer and modulus of their FFT represented by 10 based $log$ function(even-numbered rows).}
\end{figure}&lt;/p&gt;

&lt;h3 id=&#34;feature-coding&#34;&gt;Feature Coding&lt;/h3&gt;

&lt;p&gt;Number of output images in the last layer equals to the amount of convolution filters, we represent discriminant features by regrouping and combining sets of outputs. All pixels in output images are converted to binaries with unit step function $S(.)$ whose output is 1 for positive input and 0 otherwise. A single threshold makes it possible for convolution results combining with each other properly and forming a more robust feature. Decimal number representing a single pixel is formed by concatenated binaries in the same position which are converted from convolutional output corresponding to a particular convolution kernel in the penultimate layer. Such reconstructed integer-valued (in range of $[0,1,&amp;hellip;,2^{S&lt;em&gt;n-1}]$) image could be expressed as $\mathscr{O}^l = \sum&lt;/em&gt;{s = 1}^{S&lt;em&gt;n}2^{s-1}S(\mathscr{I}^l*W&lt;/em&gt;{s}^{n})$ where $s$ is the id of sets, $\mathscr{I}^l$ and $\mathscr{O}^l$ are a pair of input and output image in the last layer corresponding to the $l$th filter in the penultimate layer.&lt;/p&gt;

&lt;p&gt;Every output image for a single scale of filter bank are partitioned into $B$ blocks for precisely statistics in histogram for images in normal view. Block histograms each with the same length of $2^{S_n}$ in an image are concatenated in a vector afterwards. In the next step, all vectors deriving from the same input image at the beginning are concatenated together in the same sequence to form the feature $f$ based on the filter scale $K_i$ which are all set as odd numbers.&lt;/p&gt;

&lt;p&gt;\begin{equation}
f_k = [[Hist(\mathscr{I}_1^1),&amp;hellip;,Hist(\mathscr{I}_B^1)],&amp;hellip;,[Hist(\mathscr{I}&lt;em&gt;1^{l&lt;/em&gt;{n-1}}),&amp;hellip;,Hist(\mathscr{I}&lt;em&gt;B^{l&lt;/em&gt;{n-1}})]]
\end{equation}&lt;/p&gt;

&lt;p&gt;All features extracted from filters in several proper scales are concatenated to represent the feature of a sample $\mathscr{F} = [f_{K&lt;em&gt;1},f&lt;/em&gt;{K&lt;em&gt;2},&amp;hellip;,f&lt;/em&gt;{K&lt;em&gt;n}$.
Based on the fact that the likelihood of concatenated features equals to the form represented as cosine distance computed by normalized feature vectors: $\sum&lt;/em&gt;{k=K_1}^{K_n}&amp;lt;\textit{Norm}(f_k^i),\textit{Norm}(f&lt;em&gt;k^j)&amp;gt;$ equals to $&amp;lt;\mathscr{F}^i,\mathscr{F}^j&amp;gt;$ where $&amp;lt;,&amp;gt;$ denotes the matrix inner product, $\mathscr{F} = [\textit{Norm}(f&lt;/em&gt;{K&lt;em&gt;1}),\textit{Norm}(f&lt;/em&gt;{K&lt;em&gt;2}),&amp;hellip;,\textit{Norm}(f&lt;/em&gt;{K_n})]$, we carried out our experiment on such manner instead of concatenating all discriminant sub-feature together for the sake of saving memory in some databases. Such process could also be carried out after feature projection using classifiers as a mean of similarity fusion.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
