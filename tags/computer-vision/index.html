<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>computer vision | Yida Wang</title>

<meta name="keywords" content="" />
<meta name="description" content="Theme PaperMod - https://github.com/adityatelange/hugo-PaperMod">
<meta name="author" content="Me">
<link rel="canonical" href="https://wangyida.github.io/tags/computer-vision/" />
<link href="https://wangyida.github.io/assets/css/stylesheet.min.d1ce48677bc3d2b81f6398ed4ae6ddf7262d885c01ffefc464636bbea61c0201.css" integrity="sha256-0c5IZ3vD0rgfY5jtSubd9yYtiFwB/&#43;/EZGNrvqYcAgE=" rel="preload stylesheet"
    as="style">

<link rel="icon" href="https://wangyida.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://wangyida.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://wangyida.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://wangyida.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://wangyida.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.83.1" />
<link rel="alternate" type="application/rss&#43;xml" href="https://wangyida.github.io/tags/computer-vision/index.xml">


<meta property="og:title" content="computer vision" />
<meta property="og:description" content="Theme PaperMod - https://github.com/adityatelange/hugo-PaperMod" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://wangyida.github.io/tags/computer-vision/" />
<meta property="og:image" content="https://wangyida.github.io/104"/>
<meta property="og:updated_time" content="2020-08-25T10:15:01+02:00" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://wangyida.github.io/104"/>

<meta name="twitter:title" content="computer vision"/>
<meta name="twitter:description" content="Theme PaperMod - https://github.com/adityatelange/hugo-PaperMod"/>



</head>

<body class="list" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<noscript>
    <style type="text/css">
        .theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://wangyida.github.io/" accesskey="h">Yida Wang</a>
            <span class="logo-switches">
                <span class="theme-toggle">
                    <a id="theme-toggle" accesskey="t">
                        <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                            fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                            stroke-linejoin="round">
                            <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                        </svg>
                        <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                            fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                            stroke-linejoin="round">
                            <circle cx="12" cy="12" r="5"></circle>
                            <line x1="12" y1="1" x2="12" y2="3"></line>
                            <line x1="12" y1="21" x2="12" y2="23"></line>
                            <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                            <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                            <line x1="1" y1="12" x2="3" y2="12"></line>
                            <line x1="21" y1="12" x2="23" y2="12"></line>
                            <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                            <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                        </svg>
                    </a>
                </span>
                
                <span class="lang-switch">
                    <span>|</span>
                    <ul>
                        <li>
                            <a href="https://wangyida.github.io/l2/" title="German"
                                aria-label="German">German</a>
                        </li>
                        <li>
                            <a href="https://wangyida.github.io/l3/" title="中文"
                                aria-label="中文">中文</a>
                        </li>
                    </ul>
                </span>
            </span>
        </div>
        <ul class="menu" id="menu" onscroll="menu_on_scroll()">
            <li>
                <a href="https://wangyida.github.io/archives/">
                    <span>
                        Archive
                    </span>
                </a>
            </li>
            <li>
                <a href="https://wangyida.github.io/tags/">
                    <span>
                        Tags
                    </span>
                </a>
            </li></ul>
    </nav>
</header>

    <main class="main"> 
<header class="page-header">
  <h1>computer vision</h1>
</header>



<article class="post-entry tag-entry"> 
<figure class="entry-cover"><img src="https://wangyida.github.io/softpoolnet.png" alt="caption for image">
</figure>

  <header class="entry-header">
    <h2>
      SoftPoolNet - Shape Descriptor for Point Cloud Completion and Classification
    </h2>
  </header>
  <section class="entry-content">
    <p>| paper | code |
  Abstrarct Point clouds are often the default choice for many applications as they exhibit more flexibility and efficiency than volumetric data. Nevertheless, their unorganized nature – points are stored in an unordered way – makes them less suited to be processed by deep learning pipelines. In this paper, we propose a method for 3D object completion and classification based on point clouds. We introduce a new way of organizing the extracted features based on their activations, which we name soft pooling....</p>
  </section>
  <footer class="entry-footer">

August 25, 2020&nbsp;·&nbsp;2 min&nbsp;·&nbsp;Me
</footer>
  <a class="entry-link" aria-label="post link to SoftPoolNet - Shape Descriptor for Point Cloud Completion and Classification" href="https://wangyida.github.io/posts/2020/youtube/"></a>
</article>
<article class="post-entry tag-entry"> 
<figure class="entry-cover"><img src="https://wangyida.github.io/forknet.png" alt="caption for image">
</figure>

  <header class="entry-header">
    <h2>
      ForkNet - Multi-Branch Volumetric Semantic Completion From a Single Depth Image
    </h2>
  </header>
  <section class="entry-content">
    <p>| paper | code |
  Abstrarct We propose a novel model for 3D semantic completion from a single depth image, based on a single encoder and three separate generators used to reconstruct different geometric and semantic representations of the original and completed scene, all sharing the same latent space. To transfer information between the geometric and semantic branches of the network, we introduce paths between them concatenating features at corresponding network layers....</p>
  </section>
  <footer class="entry-footer">

November 1, 2019&nbsp;·&nbsp;2 min&nbsp;·&nbsp;Me
</footer>
  <a class="entry-link" aria-label="post link to ForkNet - Multi-Branch Volumetric Semantic Completion From a Single Depth Image" href="https://wangyida.github.io/posts/2019/youtube/"></a>
</article>
<article class="post-entry tag-entry"> 
<figure class="entry-cover"><img src="https://wangyida.github.io/vo-hand.png" alt="caption for image">
</figure>

  <header class="entry-header">
    <h2>
      Variational Object-aware 3D Hand Pose from a Single RGB Image
    </h2>
  </header>
  <section class="entry-content">
    <p>| paper | code |
  Abstrarct We propose an approach to estimate the 3D pose of a human hand while grasping objects from a single RGB image. Our approach is based on a probabilistic model implemented with deep architectures, which is used for regressing, respectively, the 2D hand joints heat maps and the 3D hand joints coordinates. We train our networks so to make our approach robust to large object- and self-occlusions, as commonly occurring with the task at hand....</p>
  </section>
  <footer class="entry-footer">

June 1, 2019&nbsp;·&nbsp;2 min&nbsp;·&nbsp;Me
</footer>
  <a class="entry-link" aria-label="post link to Variational Object-aware 3D Hand Pose from a Single RGB Image" href="https://wangyida.github.io/posts/2019_1/youtube/"></a>
</article>
<article class="post-entry tag-entry"> 
<figure class="entry-cover"><img src="https://wangyida.github.io/3dv-18.png" alt="caption for image">
</figure>

  <header class="entry-header">
    <h2>
      Adversarial Semantic Scene Completion from a Single Depth Image
    </h2>
  </header>
  <section class="entry-content">
    <p>| paper | code |
  Abstrarct We propose a method to reconstruct, complete and semantically label a 3D scene from a single input depth image. We improve the accuracy of the regressed semantic 3D maps by a novel architecture based on adversarial learning. In particular, we suggest using multiple adversarial loss terms that not only enforce realistic outputs with respect to the ground truth, but also an effective embedding of the internal features....</p>
  </section>
  <footer class="entry-footer">

October 9, 2018&nbsp;·&nbsp;1 min&nbsp;·&nbsp;Me
</footer>
  <a class="entry-link" aria-label="post link to Adversarial Semantic Scene Completion from a Single Depth Image" href="https://wangyida.github.io/posts/2018/youtube/"></a>
</article>
<article class="post-entry tag-entry"> 

  <header class="entry-header">
    <h2>
      Generative Model with Coordinate Metric Learning for Object Recognition Based on 3D Models
    </h2>
  </header>
  <section class="entry-content">
    <p>Abstrarct Collecting data for deep learning is so tedious which makes it hard to establish a perfect database. In this paper, we propose a generative model trained with synthetic images rendered from 3D models which can reduce the burden on collecting real training data and make the background conditions more sundry. Our architecture is composed of two subnetworks: semantic foreground object reconstruction network based on Bayesian inference and classification network based on multi-triplet cost training for avoiding over-fitting on monotone synthetic object surface and utilizing accurate informations of synthetic images like object poses and lightning conditions which are helpful for recognizing regular photos....</p>
  </section>
  <footer class="entry-footer">

November 15, 2017&nbsp;·&nbsp;2 min&nbsp;·&nbsp;Me
</footer>
  <a class="entry-link" aria-label="post link to Generative Model with Coordinate Metric Learning for Object Recognition Based on 3D Models" href="https://wangyida.github.io/posts/2017/youtube/"></a>
</article>
<article class="post-entry tag-entry"> 

  <header class="entry-header">
    <h2>
      Zigzagnet- Efficient deep learning for real object recognition based on 3D models
    </h2>
  </header>
  <section class="entry-content">
    <p>Abstrarct Effective utilization on texture-less 3D models for deep learning is significant to recognition on real photos. We eliminate the reliance on massive real training data by modifying convolutional neural network in 3 aspects: synthetic data rendering for training data generation in large quantities, multi-triplet cost function modification for multi-task learning and compact micro architecture design for producing tiny parametric model while overcoming over-fit problem in texture-less models. Network is initiated with multi-triplet cost function establishing sphere-like distribution of descriptors in each category which is helpful for recognition on regular photos according to pose, lighting condition, background and category information of rendered images....</p>
  </section>
  <footer class="entry-footer">

September 1, 2016&nbsp;·&nbsp;2 min&nbsp;·&nbsp;Me
</footer>
  <a class="entry-link" aria-label="post link to Zigzagnet- Efficient deep learning for real object recognition based on 3D models" href="https://wangyida.github.io/posts/2016_1/youtube/"></a>
</article>
<article class="post-entry tag-entry"> 

  <header class="entry-header">
    <h2>
      Self-restraint Object Recognition by Model Based CNN Learning
    </h2>
  </header>
  <section class="entry-content">
    <p>Abstrarct CNN has shown excellent performance on object recognition based on huge amount of real images. For training with synthetic data rendered from 3D models alone to reduce the workload of collecting real images, we propose a concatenated self-restraint learning structure lead by a triplet and softmax jointed loss function for object recognition. Locally connected auto encoder trained from rendered images with and without background used for object reconstruction against environment variables produces an additional channel automatically concatenated to RGB channels as input of classification network....</p>
  </section>
  <footer class="entry-footer">

April 1, 2016&nbsp;·&nbsp;1 min&nbsp;·&nbsp;Me
</footer>
  <a class="entry-link" aria-label="post link to Self-restraint Object Recognition by Model Based CNN Learning" href="https://wangyida.github.io/posts/2016/youtube/"></a>
</article>

    </main><footer class="footer">
    <span>&copy; 2021 <a href="https://wangyida.github.io/">Yida Wang</a></span>
    <span>&middot;</span>
    <span>Powered by <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a></span>
    <span>&middot;</span>
    <span>Theme <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a></span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top" accesskey="g">
    <button class="top-link" id="top-link" type="button">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6">
            <path d="M12 6H0l6-6z" /></svg>
    </button>
</a>



<script>
    window.onload = function () {
        if (localStorage.getItem("menu-scroll-position")) {
            document.getElementById('menu').scrollLeft = localStorage.getItem("menu-scroll-position");
        }
    }
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                behavior: "smooth"
            });
        });
    });
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

    function menu_on_scroll() {
        localStorage.setItem("menu-scroll-position", document.getElementById('menu').scrollLeft);
    }

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>

</body>

</html>
