<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>3D completion on Yida Wang</title>
    <link>https://wangyida.github.io/tags/3d-completion/</link>
    <description>Recent content in 3D completion on Yida Wang</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Tue, 25 Aug 2020 10:15:01 +0200</lastBuildDate><atom:link href="https://wangyida.github.io/tags/3d-completion/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>SoftPoolNet - Shape Descriptor for Point Cloud Completion and Classification</title>
      <link>https://wangyida.github.io/posts/youtube/</link>
      <pubDate>Tue, 25 Aug 2020 10:15:01 +0200</pubDate>
      
      <guid>https://wangyida.github.io/posts/youtube/</guid>
      <description>Abstrarct Point clouds are often the default choice for many applications as they exhibit more flexibility and efficiency than volumetric data. Nevertheless, their unorganized nature &amp;ndash; points are stored in an unordered way &amp;ndash; makes them less suited to be processed by deep learning pipelines. In this paper, we propose a method for 3D object completion and classification based on point clouds. We introduce a new way of organizing the extracted features based on their activations, which we name soft pooling.</description>
    </item>
    
    <item>
      <title>ForkNet - Multi-Branch Volumetric Semantic Completion From a Single Depth Image</title>
      <link>https://wangyida.github.io/posts/youtube/</link>
      <pubDate>Fri, 01 Nov 2019 10:15:01 +0200</pubDate>
      
      <guid>https://wangyida.github.io/posts/youtube/</guid>
      <description>Abstrarct We propose a novel model for 3D semantic completion from a single depth image, based on a single encoder and three separate generators used to reconstruct different geometric and semantic representations of the original and completed scene, all sharing the same latent space. To transfer information between the geometric and semantic branches of the network, we introduce paths between them concatenating features at corresponding network layers. Motivated by the limited amount of training samples from real scenes, an interesting attribute of our architecture is the capacity to supplement the existing dataset by generating a new training dataset with high quality, realistic scenes that even includes occlusion and real noise.</description>
    </item>
    
    <item>
      <title>Adversarial Semantic Scene Completion from a Single Depth Image</title>
      <link>https://wangyida.github.io/posts/youtube/</link>
      <pubDate>Tue, 09 Oct 2018 10:15:01 +0200</pubDate>
      
      <guid>https://wangyida.github.io/posts/youtube/</guid>
      <description>Abstrarct We propose a method to reconstruct, complete and semantically label a 3D scene from a single input depth image. We improve the accuracy of the regressed semantic 3D maps by a novel architecture based on adversarial learning. In particular, we suggest using multiple adversarial loss terms that not only enforce realistic outputs with respect to the ground truth, but also an effective embedding of the internal features.</description>
    </item>
    
  </channel>
</rss>
