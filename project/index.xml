<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects on Yida Wang</title>
    <link>https://wangyida.github.io/project/</link>
    <description>Recent content in Projects on Yida Wang</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Yida Wang</copyright>
    <lastBuildDate>Wed, 27 Apr 2016 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://wangyida.github.io/project/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Compact Network</title>
      <link>https://wangyida.github.io/project/compact-network/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://wangyida.github.io/project/compact-network/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Computer Vision</title>
      <link>https://wangyida.github.io/project/computer-vision/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://wangyida.github.io/project/computer-vision/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Deep Alternatives</title>
      <link>https://wangyida.github.io/project/deep-alternatives/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://wangyida.github.io/project/deep-alternatives/</guid>
      <description>Deep structure theories and replacement for deep structures.</description>
    </item>
    
    <item>
      <title>Deep Learning</title>
      <link>https://wangyida.github.io/project/deep-learning/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://wangyida.github.io/project/deep-learning/</guid>
      <description>Deep learning (also known as deep structured learning or hierarchical learning) is part of a broader family of machine learning methods based on learning data representations, as opposed to task-specific algorithms. Learning can be supervised, partially supervised or unsupervised.
Some representations are loosely based on interpretation of information processing and communication patterns in a biological nervous system, such as neural coding that attempts to define a relationship between various stimuli and associated neuronal responses in the brain.</description>
    </item>
    
    <item>
      <title>Metric Learning</title>
      <link>https://wangyida.github.io/project/metric-learning/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://wangyida.github.io/project/metric-learning/</guid>
      <description>Metric learning is the task of learning a distance function over objects. A metric or distance function has to obey four axioms: non-negativity, Identity of indiscernibles, symmetry and subadditivity / triangle inequality. In practice, metric learning algorithms ignore the condition of identity of indiscernibles and learn a pseudo-metric.</description>
    </item>
    
    <item>
      <title>Open Source</title>
      <link>https://wangyida.github.io/project/open-source/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://wangyida.github.io/project/open-source/</guid>
      <description>Google Open Source:  Google summer of Codes 2015: Deep Learning &amp;amp; GPU Implementation for Metric Learning for OpenCV Google summer of Codes 2016: tiny-dnn &amp;amp; OpenCV
 Microsoft Research: 2nd prize of Microsoft Open Source Challenge 2016: CNTK on Mac: 2D Object Restoration and Recognition on 3D Model
 Scilab: 1st prize of Scilab Scientific Simulation Contest 2013: AIS System
 CSDN: CSDN Summer of Codes: BladeRF Personal Telecommuncation Base</description>
    </item>
    
    <item>
      <title>Telecommunication</title>
      <link>https://wangyida.github.io/project/telecommunication/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://wangyida.github.io/project/telecommunication/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Variational Inference</title>
      <link>https://wangyida.github.io/project/variational-inference/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://wangyida.github.io/project/variational-inference/</guid>
      <description>Variational Bayesian methods are a family of techniques for approximating intractable integrals arising in Bayesian inference and machine learning. They are typically used in complex statistical models consisting of observed variables (usually termed &amp;ldquo;data&amp;rdquo;) as well as unknown parameters and latent variables, with various sorts of relationships among the three types of random variables, as might be described by a graphical model. As is typical in Bayesian inference, the parameters and latent variables are grouped together as &amp;ldquo;unobserved variables&amp;rdquo;.</description>
    </item>
    
  </channel>
</rss>